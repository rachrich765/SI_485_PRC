{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn pdf into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdf_to_text(file):\n",
    "    pdf_file = open(file, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    page_content = []\n",
    "    for i in range(0, number_of_pages):\n",
    "        page = read_pdf.getPage(i)\n",
    "        page = page.extractText()\n",
    "        page = page.lower()\n",
    "        page_content.append(page)\n",
    "    page_content = ''.join(page_content)\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt to brute-force locate notification format headers. use the headers to find the relevant paragraph with information on the source of the breach and the data type breached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_notice_headers_source(text):\n",
    "    text_format = text\n",
    "    text = text.replace('\\n','')\n",
    "    notice_headers = ['what happened','what we are doing', 'what information was involved', 'what you can do', 'for more information']\n",
    "    working = [True for x in notice_headers if x in text]\n",
    "    if working:\n",
    "        k = text.index(notice_headers[0])\n",
    "        try:\n",
    "            l = text.index(notice_headers[1])\n",
    "        except:\n",
    "            l = text.index(notice_headers[2])\n",
    "        source = text[k:l]        \n",
    "        return source\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_notice_headers_datatype(text):\n",
    "    text_format = text\n",
    "    text = text.replace('\\n','')\n",
    "    notice_headers = ['what happened','what we are doing', 'what information was involved', 'what you can do', 'for more information']\n",
    "    working = [True for x in notice_headers if x in text]\n",
    "    if working:\n",
    "        k = text.index(notice_headers[3])\n",
    "        try:\n",
    "            l = text.index(notice_headers[4])\n",
    "        except:\n",
    "            l = text.index(notice_headers[5])\n",
    "        source = text[k:l]        \n",
    "        return source\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the output dictionary\n",
    "From the above functions I created two dictionaries, each mapping the document number (key) to the document string under the appropriate header (value). I then cleaned these dictionaries to produce wordcounts of the overall paragraphs to use for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dict(dct):\n",
    "    dct_refined = dct.copy()\n",
    "    for key in dct:\n",
    "        if type(dct[key]) == bool:\n",
    "            del dct_refined[key]\n",
    "    for key in dct:\n",
    "        try:\n",
    "            if len(key)<20:\n",
    "                del dct_refined[key]\n",
    "        except:\n",
    "            continue\n",
    "    return dct_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a counter to count number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_counter(source_dct):\n",
    "    l = []\n",
    "    for key in source_dct:\n",
    "        st = source_dct[key]\n",
    "        words = nltk.word_tokenize(st)\n",
    "        l.extend(words)\n",
    "    word_counts = Counter(l)\n",
    "    for item in list(string.punctuation):\n",
    "            if item in word_counts:\n",
    "                del word_counts[item]\n",
    "    for item in stop_words:\n",
    "            if item in word_counts:\n",
    "                del word_counts[item]\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
